{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertModel, BertConfig\n",
    "from transformers import XLMRobertaModel\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving to log files\n",
    "\n",
    "logging.basicConfig(filename=f'./logs/mBERT_{time.asctime().replace(\" \",\"_\")}.log', filemode='w', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Create a logger object\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create a stream handler to print log messages to the console\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "\n",
    "dt=pd.read_csv('/data1/aakash/Codemix/Aakash_02/Datasets/1HateSpeech_Codemix.csv') #codemix\n",
    "dt=dt.dropna()\n",
    "\n",
    "# train_df, remaining_df = train_test_split(dt, test_size=0.3, random_state=random_seed, stratify=dt['Tag'])\n",
    "# test_df, val_df = train_test_split(remaining_df, test_size=0.5, random_state=random_seed, stratify=remaining_df['Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dy = pd.read_csv('/data1/aakash/Codemix/Aakash_02/Datasets/1Hatespeech_English(new).csv')\n",
    "\n",
    "\n",
    "\n",
    "dx = pd.read_csv('/data1/aakash/Codemix/Aakash_02/Datasets/1HateSpeech_Hindi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_com = pd.concat([dt,dy,dx])\n",
    "hate_com = hate_com.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = hate_com['Tag'].replace({1:999, 0:999})\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "\n",
    "dn=pd.read_csv('/data1/aakash/Codemix/Aakash_02/Datasets/1Humour_Codemix.csv') #codemix\n",
    "dn=dn.dropna()\n",
    "\n",
    "\n",
    "dm = pd.read_csv('/data1/aakash/Codemix/Aakash_02/Datasets/1Humour_English(NEW).csv')\n",
    "dm = dm[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_com = pd.concat([dn,dm])\n",
    "sar_com = sar_com.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=sar_com['Tag']\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sar_com['Tag']= sar_com['Tag'].replace({1:999, 0:999})\n",
    "sar_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = sar_com['Tag'].replace({1:999, 0:999})\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/data1/aakash/Codemix/Aakash_02/Datasets/1Sarcasm_Codemix.csv')\n",
    "\n",
    "train_df, remaining_df = train_test_split(df, test_size=0.3, random_state=random_seed, stratify=df['Tag'])\n",
    "test_df, val_df = train_test_split(remaining_df, test_size=0.5, random_state=random_seed, stratify=remaining_df['Tag'])\n",
    "\n",
    "\n",
    "dz = pd.read_csv('/data1/aakash/Codemix/Aakash_02/Datasets/1Sarcasm_English.csv')\n",
    "dz = dz[:3000]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_com = pd.concat([train_df,dz])\n",
    "hum_com = hum_com.reset_index(drop=True)\n",
    "hum_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=hum_com['Tag']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=pd.concat([z,n,y])\n",
    "s=s.reset_index(drop=True)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hum_com['Tag']= hum_com['Tag'].replace({1:999, 0:999})\n",
    "hum_com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = hum_com['Tag'].replace({1:999, 0:999})\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.concat([z,k,r])\n",
    "x=x.reset_index(drop=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_1 = pd.concat([hate_com,sar_com,hum_com])\n",
    "com_1 = com_1.reset_index(drop=True)\n",
    "com_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_1['Task2'] = s\n",
    "com_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_1['Task3'] = x\n",
    "com_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "com_1 = com_1.rename(columns={'Tag': 'Task1'})\n",
    "com_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the DataFrame\n",
    "data_tr = com_1.sample(frac=1, random_state=42)\n",
    "data_tr.reset_index(drop=True, inplace=True)\n",
    "data_tr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = val_df\n",
    "data_val = data_val.reset_index(drop=True)\n",
    "data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "data_val = data_val.rename(columns={'Tag': 'Task1'})\n",
    "data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val['Task2'] = 999\n",
    "data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interchange columns\n",
    "data_val['Task1'], data_val['Task2'] = data_val['Task2'].copy(), data_val['Task1'].copy()\n",
    "data_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val['Task3'] = 999\n",
    "data_val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = test_df\n",
    "data_test = data_test.reset_index(drop=True)\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column\n",
    "data_test = data_test.rename(columns={'Tag': 'Task1'})\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Task2'] = 999\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interchange columns\n",
    "data_test['Task1'], data_test['Task2'] = data_test['Task2'].copy(), data_test['Task1'].copy()\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['Task3'] = 999\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedCrossTaskModel(nn.Module):\n",
    "    def __init__(self, num_classes_task1, num_classes_task2, num_classes_task3):\n",
    "        super(SharedCrossTaskModel, self).__init__()\n",
    "\n",
    "        # Load a pre-trained BERT model\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased',output_hidden_states=True)\n",
    "        #self.bert = XLMRobertaModel.from_pretrained('xlm-roberta-base',output_hidden_states=True)\n",
    "        \n",
    "\n",
    "        # Task-specific layers for Task 1\n",
    "        self.task1_specific_layers = nn.ModuleList([deepcopy(self.bert.encoder.layer[i]) for i in range(-4, 0, 1)])\n",
    "\n",
    "\n",
    "        # Task-specific layers for Task 2\n",
    "        self.task2_specific_layers = nn.ModuleList([deepcopy(self.bert.encoder.layer[i]) for i in range(-4, 0, 1)])\n",
    "\n",
    "\n",
    "        # Task-specific layers for Task 3\n",
    "        self.task3_specific_layers = nn.ModuleList([deepcopy(self.bert.encoder.layer[i]) for i in range(-4, 0, 1)])\n",
    "\n",
    "\n",
    "        # setting the final gating mechanism\n",
    "        self.gating_modules_task1 = Gating(2, self.bert.config.hidden_size)\n",
    "        self.gating_modules_task2 = Gating(2, self.bert.config.hidden_size)\n",
    "        self.gating_modules_task3 = Gating(2, self.bert.config.hidden_size)\n",
    "\n",
    "\n",
    "        self.num_classes_task1 = num_classes_task1\n",
    "        self.num_classes_task2 = num_classes_task2\n",
    "        self.num_classes_task3 = num_classes_task3\n",
    "\n",
    "\n",
    "\n",
    "        # Task-specific classifiers\n",
    "        self.classifier_task1 = nn.Linear(self.bert.config.hidden_size, num_classes_task1)\n",
    "        self.classifier_task2 = nn.Linear(self.bert.config.hidden_size, num_classes_task2)\n",
    "        self.classifier_task3 = nn.Linear(self.bert.config.hidden_size, num_classes_task3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \n",
    "        \"\"\" \n",
    "        shape of input_ids: [batch_size, seq_len]\n",
    "        shape of attention_mask: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "\n",
    "        # BERT Output\n",
    "        bert_output = self.bert(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        hidden_states = bert_output.hidden_states\n",
    "\n",
    "        #print(hidden_states[:8])\n",
    "\n",
    "\n",
    "        # Task-specific layers for Task 1\n",
    "        current_output1 = hidden_states[7]\n",
    "        \n",
    "        for layer in self.task1_specific_layers:\n",
    "            current_output1 = layer(current_output1)[0]\n",
    "\n",
    "\n",
    "        # Task-specific layers for Task 2\n",
    "        current_output2 = hidden_states[7]\n",
    "        \n",
    "        for layer in self.task2_specific_layers:\n",
    "            current_output2 = layer(current_output2)[0]\n",
    "\n",
    "\n",
    "\n",
    "       # Task-specific layers for Task 3\n",
    "        current_output3 = hidden_states[7]\n",
    "\n",
    "        for layer in self.task3_specific_layers:\n",
    "            current_output3 = layer(current_output3)[0]     \n",
    "\n",
    "\n",
    "        # print(current_output1.shape)\n",
    "        # print(bert_output.last_hidden_state[-4:].shape)\n",
    "\n",
    "\n",
    "        # Combined output after gating\n",
    "        combined_output_task1 = self.gating_modules_task1(tuple([hidden_states[-1], current_output1]))\n",
    "        combined_output_task2 = self.gating_modules_task2(tuple([hidden_states[-1], current_output2]))\n",
    "        combined_output_task3 = self.gating_modules_task3(tuple([hidden_states[-1], current_output3]))\n",
    "\n",
    "\n",
    "        pooled_output_task1= combined_output_task1[:,0,:]\n",
    "        pooled_output_task1=torch.squeeze(pooled_output_task1,dim=1)\n",
    "\n",
    "        pooled_output_task2= combined_output_task2[:,0,:]\n",
    "        pooled_output_task2=torch.squeeze(pooled_output_task2,dim=1)\n",
    "\n",
    "\n",
    "        pooled_output_task3= combined_output_task3[:,0,:]\n",
    "        pooled_output_task3=torch.squeeze(pooled_output_task3,dim=1)\n",
    "        \n",
    "\n",
    "        # Task-specific classifiers\n",
    "        logits_task1 = self.classifier_task1(pooled_output_task1)\n",
    "        logits_task2 = self.classifier_task2(pooled_output_task2)\n",
    "        logits_task3 = self.classifier_task2(pooled_output_task3)\n",
    "        \n",
    "        return logits_task1, logits_task2, logits_task3\n",
    "    \n",
    "        \n",
    "class Gating(torch.nn.Module):\n",
    "    def __init__(self, num_gates, input_dim):\n",
    "        super(Gating, self).__init__()\n",
    "        self.num_gates = num_gates\n",
    "        self.input_dim = input_dim\n",
    "        if self.num_gates == 2:\n",
    "            self.linear = torch.nn.Linear(self.num_gates * self.input_dim, self.input_dim)\n",
    "        elif self.num_gates > 2:\n",
    "            self.linear = torch.nn.Linear(self.num_gates * self.input_dim, self.num_gates * self.input_dim)\n",
    "            self.softmax = torch.nn.Softmax(-1)\n",
    "        else:\n",
    "            raise ValueError('num_gates should be greater or equal to 2')\n",
    "\n",
    "    def forward(self, tuple_of_inputs):\n",
    "        # output size should be equal to the input sizes\n",
    "        if self.num_gates == 2:\n",
    "            #print(\"Tensor Sizes Before Concatenation:\", tuple_of_inputs[0].size(), tuple_of_inputs[1].size())\n",
    "            alpha = torch.sigmoid(self.linear(torch.cat(tuple_of_inputs, dim=-1)))\n",
    "            output = torch.mul(alpha, tuple_of_inputs[0]) + torch.mul(1 - alpha, tuple_of_inputs[1])\n",
    "        else:  # elif self.num_gates > 2:\n",
    "            # extend the gating mechanism to more than 2 encoders\n",
    "            batch_size, len_size, dim_size = tuple_of_inputs[0].size()\n",
    "            alpha = torch.sigmoid(self.linear(torch.cat(tuple_of_inputs, dim=-1)))\n",
    "            alpha = self.softmax(alpha.view(batch_size, len_size, dim_size, self.num_gates))\n",
    "            output = torch.sum(torch.mul(alpha, torch.stack(tuple_of_inputs, dim=-1)), dim=-1)\n",
    "        return output\n",
    "    \n",
    "\n",
    "# Instantiate the model\n",
    "model = SharedCrossTaskModel(num_classes_task1 = 2, num_classes_task2 = 2, num_classes_task3 = 2)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "#tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, sentences, labels1, labels2, labels3, tokenizer, max_len):\n",
    "        self.sentences = sentences\n",
    "        self.labels1 = labels1\n",
    "        self.labels2 = labels2\n",
    "        self.labels3 = labels3\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        #self.task_tokens = task_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            sentence = str(self.sentences[idx])\n",
    "            label1 = self.labels1[idx]\n",
    "            label2 = self.labels2[idx]\n",
    "            label3 = self.labels3[idx]\n",
    "\n",
    "            # Tokenize the input sentence with task tokens added as special tokens\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                sentence,  \n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_len,  # Adjust max length without subtracting task tokens\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'label1': torch.tensor(label1, dtype=torch.long),\n",
    "                'label2': torch.tensor(label2, dtype=torch.long),\n",
    "                'label3': torch.tensor(label3, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Problematic sentence: {self.sentences[idx]}\")\n",
    "            print(f\"Problematic label1: {self.labels1[idx]}\")\n",
    "            print(f\"Problematic label2: {self.labels2[idx]}\")\n",
    "            print(f\"Problematic label3: {self.labels3[idx]}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128 \n",
    "batch_size = 64\n",
    "epochs = 50\n",
    "learning_rate = 5e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(data_tr['Sentence'].values, data_tr['Task1'].values, data_tr['Task2'].values, data_tr['Task3'].values, tokenizer, max_len)\n",
    "val_dataset = CustomDataset(data_val['Sentence'].values, data_val['Task1'].values, data_val['Task2'].values, data_val['Task3'].values, tokenizer, max_len)\n",
    "test_dataset = CustomDataset(data_test['Sentence'].values, data_test['Task1'].values, data_test['Task2'].values, data_test['Task3'].values, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=32)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def custom_joint_loss(logits_task1, logits_task2, logits_task3, labels_task1, labels_task2, labels_task3):\n",
    "    # Calculate the loss for task 2, but only for rows where task1 == 999 and task3 == 999\n",
    "    mask_task2 = (labels_task1 == 999) & (labels_task3 == 999)\n",
    "    loss_task2 = nn.CrossEntropyLoss()(logits_task2[mask_task2], labels_task2[mask_task2])\n",
    "\n",
    "    # Calculate the loss for task 1, but only for rows where task2 == 999 and task3 == 999\n",
    "    mask_task11 = (labels_task2 == 999)\n",
    "    mask_task1 = (labels_task2 == 999) & (labels_task3 == 999)\n",
    "    if torch.sum(mask_task11) == 0:\n",
    "        loss_task1_2 = 0.0  # Set the loss to 0 when there are no rows with task2 == 999 \n",
    "    else:\n",
    "        loss_task1_2 = nn.CrossEntropyLoss()(logits_task1[mask_task1], labels_task1[mask_task1])\n",
    "\n",
    "    # Calculate the loss for task 3, but only for rows where task1 == 999 and task2 == 999\n",
    "    mask_task3 = (labels_task1 == 999) & (labels_task2 == 999)\n",
    "    if torch.sum(mask_task11) == 0:\n",
    "        loss_task3 = 0.0\n",
    "    else:\n",
    "        loss_task3 = nn.CrossEntropyLoss()(logits_task3[mask_task3], labels_task3[mask_task3])\n",
    "\n",
    "    # Combine the losses\n",
    "    joint_loss = loss_task2 + loss_task1_2 + loss_task3\n",
    "\n",
    "    return joint_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "best_val_f1_score = 0\n",
    "patience = 4  # Number of epochs to wait for improvement\n",
    "counter = 0  # Counter to keep track of epochs without improvement\n",
    "\n",
    "\n",
    "#best_model_save_path\n",
    "tempdir = '/data1/aakash/Codemix/Aakash_02/.model/'\n",
    "best_model_params_path = os.path.join(tempdir, f\"mbert_Com.pt\")\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_task1 = batch['label1'].to(device)\n",
    "        labels_task2 = batch['label2'].to(device)\n",
    "        labels_task3 = batch['label3'].to(device)\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits_task1, logits_task2, logits_task3 = model(input_ids, attention_mask=attention_mask)  # Get logits for all tasks\n",
    "\n",
    "        loss = custom_joint_loss(logits_task1, logits_task2, logits_task3, labels_task1, labels_task2, labels_task3)  # Use custom joint loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    logging.info(f'Train loss: {avg_train_loss:.4f}')\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "\n",
    "    # Validation\n",
    "\n",
    "    #criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.eval()\n",
    "    val_predictions_task1 = []\n",
    "    val_predictions_task2 = []\n",
    "    val_predictions_task3 = []\n",
    "    val_labels_task1 = []\n",
    "    val_labels_task2 = []\n",
    "    val_labels_task3 = []\n",
    "    val_loss = 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels_task1 = batch['label1'].to(device)\n",
    "        labels_task2 = batch['label2'].to(device)\n",
    "        labels_task3 = batch['label3'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_task1, logits_task2, logits_task3 = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        predicted_labels_task1 = torch.argmax(logits_task1, dim=1)\n",
    "        predicted_labels_task2 = torch.argmax(logits_task2, dim=1)\n",
    "        predicted_labels_task3 = torch.argmax(logits_task3, dim=1)\n",
    "\n",
    "        val_predictions_task1.extend(predicted_labels_task1.detach().cpu().numpy())\n",
    "        val_predictions_task2.extend(predicted_labels_task2.detach().cpu().numpy())\n",
    "        val_predictions_task3.extend(predicted_labels_task3.detach().cpu().numpy())\n",
    "        val_labels_task1.extend(labels_task1.detach().cpu().numpy())\n",
    "        val_labels_task2.extend(labels_task2.detach().cpu().numpy())\n",
    "        val_labels_task3.extend(labels_task3.detach().cpu().numpy())\n",
    "\n",
    "        # Use custom joint loss for validation loss calculation\n",
    "        val_loss += custom_joint_loss(logits_task1, logits_task2, logits_task3, labels_task1, labels_task2, labels_task3).item()\n",
    "\n",
    "\n",
    "    epoch_val_accuracy = accuracy_score(val_labels_task2, val_predictions_task2)\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    #print(f'Validation accuracy: {epoch_val_accuracy:.4f}')\n",
    "    logging.info(f'Validation loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    val_f1_score = f1_score(val_labels_task2, val_predictions_task2, average='macro')\n",
    "\n",
    "    classification_report_epoch = classification_report(val_labels_task2, val_predictions_task2)\n",
    "    logging.info(f'Classification Report per Epoch {epoch+1}:')\n",
    "    logging.info(classification_report_epoch)\n",
    "    \n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_params_path) # Saving best model\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1}')\n",
    "            break\n",
    "\n",
    "    # if val_f1_score > best_val_f1_score:\n",
    "    #     best_val_f1_score = val_f1_score\n",
    "    #     torch.save(model.state_dict(), best_model_params_path)\n",
    "    #     counter = 0\n",
    "    # else:\n",
    "    #     counter += 1\n",
    "    #     if counter >= patience:\n",
    "    #         print(f'Early stopping at epoch {epoch + 1}')\n",
    "    #         break\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_eval_accuracy = 0\n",
    "\n",
    "test_predictions_task1 = []\n",
    "test_predictions_task2 = []\n",
    "test_predictions_task3 = []\n",
    "true_labels_task1 = []\n",
    "true_labels_task2 = []\n",
    "true_labels_task3 = []\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels_task1 = batch['label1'].to(device)\n",
    "    labels_task2 = batch['label2'].to(device)\n",
    "    labels_task3 = batch['label3'].to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_task1, logits_task2, logits_task3 = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    #logits = outputs.logits\n",
    "    predicted_lab_task1 = torch.argmax(logits_task1, dim=1)\n",
    "    predicted_lab_task2 = torch.argmax(logits_task2, dim=1)\n",
    "    predicted_lab_task3 = torch.argmax(logits_task3, dim=1)\n",
    "\n",
    "    # accuracy = (predicted_labels == labels).float().mean()\n",
    "    # total_eval_accuracy += accuracy.item()\n",
    "\n",
    "    test_predictions_task1.extend(predicted_lab_task1.detach().cpu().numpy())\n",
    "    test_predictions_task2.extend(predicted_lab_task2.detach().cpu().numpy())\n",
    "    test_predictions_task3.extend(predicted_lab_task3.detach().cpu().numpy())\n",
    "    true_labels_task1.extend(labels_task1.detach().cpu().numpy())\n",
    "    true_labels_task2.extend(labels_task2.detach().cpu().numpy())\n",
    "    true_labels_task3.extend(labels_task3.detach().cpu().numpy())\n",
    "\n",
    "\n",
    "\n",
    "# avg_test_accuracy = total_eval_accuracy / len(test_dataloader)\n",
    "# logging.info(f'Test accuracy: {avg_test_accuracy:.4f}')\n",
    "\n",
    "\n",
    "classification_report_output = classification_report(true_labels_task2, test_predictions_task2,zero_division=0,digits=6)\n",
    "\n",
    "logging.info('Classification Report:')\n",
    "logging.info(classification_report_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1f4bdd0e0e482ea9d8fde0a5605220594084c9ef6ce9afd4984964d2f5d1efc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
