{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/debajyoti/test/llms/envllm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,TrainingArguments\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "if USE_GPU:\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# you may want to change the following parameters depending on your GPU configuration\n",
    "\n",
    "# free T4 instance\n",
    "# QUANTIZE_4BIT = True\n",
    "# USE_GRAD_CHECKPOINTING = True\n",
    "# TRAIN_BATCH_SIZE = 2\n",
    "# TRAIN_MAX_SEQ_LENGTH = 512\n",
    "# USE_FLASH_ATTENTION = False\n",
    "# GRAD_ACC_STEPS = 16\n",
    "\n",
    "# equivalent A100 setting\n",
    "QUANTIZE_4BIT = True\n",
    "USE_GRAD_CHECKPOINTING = True\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TRAIN_MAX_SEQ_LENGTH = 512\n",
    "USE_FLASH_ATTENTION = True\n",
    "GRAD_ACC_STEPS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.33s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"CohereForAI/aya-expanse-8b\"\n",
    "models_path = \"/data1/debajyoti/test/llms/models/\"\n",
    "\n",
    "attn_implementation = None\n",
    "if USE_FLASH_ATTENTION:\n",
    "  attn_implementation=\"flash_attention_2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "          MODEL_NAME,\n",
    "          attn_implementation=attn_implementation,\n",
    "          torch_dtype=torch.bfloat16,\n",
    "          cache_dir=models_path,\n",
    "          device_map=\"auto\"\n",
    "        )\n",
    "# model = model.to(device)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_message_format(prompts):\n",
    "  messages = []\n",
    "\n",
    "  for p in prompts:\n",
    "    messages.append(\n",
    "          [{\"role\": \"system\", \"content\": \"\"\"You are a humor recognition assistant judging if an 'Input' is humorous or not. No need to consider any prior context.\n",
    "            If the 'Input' is humorous, you need to output your final 'Output' as 'Humor'. \n",
    "            If the 'Input' is non-humorous, you need to output your final 'Output' as 'Non-humor'.\"\"\"},\n",
    "          {\"role\": \"user\", \"content\": p}]\n",
    "      )\n",
    "\n",
    "  return messages\n",
    "\n",
    "def generate_aya(\n",
    "      model,\n",
    "      prompts,\n",
    "      temperature=0.75,\n",
    "      top_p=1.0,\n",
    "      top_k=0,\n",
    "      max_new_tokens=1024\n",
    "    ):\n",
    "\n",
    "  # print(prompts)\n",
    "  messages = get_message_format(prompts)\n",
    "  # print(messages)\n",
    "\n",
    "  input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "      )\n",
    "  input_ids = input_ids.to(model.device)\n",
    "  prompt_padded_len = len(input_ids[0])\n",
    "\n",
    "  gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "      )\n",
    "\n",
    "  # get only generated tokens\n",
    "  gen_tokens = [\n",
    "      gt[prompt_padded_len:] for gt in gen_tokens\n",
    "    ]\n",
    "\n",
    "  gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)\n",
    "  return gen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Input: \"Chalo protocol pata chal gaya. Vice President agar Muslim na hote toh shayad kabhi pata hi nahi\" \\n    Output: ']\n",
      "[[{'role': 'system', 'content': \"You are a humor recognition assistant judging if an 'Input' is humorous or not. No need to consider any prior context.\\n            If the 'Input' is humorous, you need to output your final 'Output' as 'Humor'. \\n            If the 'Input' is non-humorous, you need to output your final 'Output' as 'Non-humor'.\"}, {'role': 'user', 'content': 'Input: \"Chalo protocol pata chal gaya. Vice President agar Muslim na hote toh shayad kabhi pata hi nahi\" \\n    Output: '}]]\n",
      "PROMPT\n",
      "Input: \"Chalo protocol pata chal gaya. Vice President agar Muslim na hote toh shayad kabhi pata hi nahi\" \n",
      "    Output: \n",
      "RESPONSE\n",
      "Output: Humor\n",
      "\n",
      "The statement plays on the idea of a hypothetical scenario where the Vice President's actions or decisions might have been different if they were not Muslim, implying a bias or lack of understanding of their own role due to religious assumptions. This is a humorous take on political and religious dynamics, as it pokes fun at the notion of using religion as a factor in decision-making.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test generations on langauges in Aya 23 set\n",
    "prompts = [\n",
    "    \"\"\"Input: \"Chalo protocol pata chal gaya. Vice President agar Muslim na hote toh shayad kabhi pata hi nahi\" \n",
    "    Output: \"\"\"\n",
    "]\n",
    "\n",
    "generations = generate_aya(model, prompts)\n",
    "\n",
    "for p, g in zip(prompts, generations):\n",
    "  print(\n",
    "      \"PROMPT\", p ,\"RESPONSE\", g, \"\\n\", sep=\"\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is 2 takiye ki naukri main mera lakhon ka worl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Naam : Viv Richards\\nBaap ka naam : Master Di...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sakshi Maharaj is BJP's Digvijaya Singh.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aaj @British_Airways ki toh lag gayi bhai.\\nHa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ghanta development hoga! Saare paise to electi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Hum to Aam Aadmi hain ji, woh Megalomaniac hai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Neend nahi aati hai raaton mein?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Ghar mein Bipasha Basu ki photo rakhne se bhoo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Aur dikhao aur dikhaopic.twitter.com/Ij4R6xNg9V</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Tu na ja mere baadshah ek waade ke liye, maa c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>296 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  Tag\n",
       "0    Is 2 takiye ki naukri main mera lakhon ka worl...    0\n",
       "1    \"Naam : Viv Richards\\nBaap ka naam : Master Di...    1\n",
       "2             Sakshi Maharaj is BJP's Digvijaya Singh.    1\n",
       "3    Aaj @British_Airways ki toh lag gayi bhai.\\nHa...    1\n",
       "4    Ghanta development hoga! Saare paise to electi...    1\n",
       "..                                                 ...  ...\n",
       "291  Hum to Aam Aadmi hain ji, woh Megalomaniac hai...    1\n",
       "292                   Neend nahi aati hai raaton mein?    0\n",
       "293  Ghar mein Bipasha Basu ki photo rakhne se bhoo...    1\n",
       "294    Aur dikhao aur dikhaopic.twitter.com/Ij4R6xNg9V    0\n",
       "295  Tu na ja mere baadshah ek waade ke liye, maa c...    1\n",
       "\n",
       "[296 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train_df_eng = pd.read_csv(\"/data1/debajyoti/code-mix-humor-sarcasm-detection/Dataset/1Humour_English(NEW).csv\")\n",
    "# train_df_eng = train_df_eng[:2000]\n",
    "# train_df_hin = pd.read_csv(\"/data1/debajyoti/code-mix-humor-sarcasm-detection/Dataset/1Humour_English(NEW).csv\")\n",
    "# train_df_hin = train_df_hin[:2000]\n",
    "# train_df = pd.concat([train_df_eng, train_df_hin], ignore_index=True)\n",
    "# train_df = train_df_hin\n",
    "train_df = pd.read_csv(\"/data1/debajyoti/code-mix-humor-sarcasm-detection/Dataset/splits/humor/train.csv\")\n",
    "val_df = pd.read_csv(\"/data1/debajyoti/code-mix-humor-sarcasm-detection/Dataset/splits/humor/val.csv\")\n",
    "test_df = pd.read_csv(\"/data1/debajyoti/code-mix-humor-sarcasm-detection/Dataset/splits/humor/test.csv\")\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TL pe koi adarsh liberal jisko patakho se dikk...</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chyamaila Hou de kharcha #Gudipadwapic.twitter...</td>\n",
       "      <td>Non-humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAP walon ke ye haal hai ki agar Modi pe joke ...</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1k rt tweets ko bhi aaj kal 5-7 rts milte hai....</td>\n",
       "      <td>Non-humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ab jake mera tweet padha bhai ne. Sala itni de...</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baa ji @BDUTT main to bolta hoon Indrani ki ja...</td>\n",
       "      <td>Non-humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bhai @imVkohli ye century pe century lagakar a...</td>\n",
       "      <td>Humor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kabse iss pyaase jhagadte TL par, sense ki ek ...</td>\n",
       "      <td>Non-humor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence        Tag\n",
       "0  TL pe koi adarsh liberal jisko patakho se dikk...      Humor\n",
       "1  Chyamaila Hou de kharcha #Gudipadwapic.twitter...  Non-humor\n",
       "2  AAP walon ke ye haal hai ki agar Modi pe joke ...      Humor\n",
       "3  1k rt tweets ko bhi aaj kal 5-7 rts milte hai....  Non-humor\n",
       "4  Ab jake mera tweet padha bhai ne. Sala itni de...      Humor\n",
       "5  Baa ji @BDUTT main to bolta hoon Indrani ki ja...  Non-humor\n",
       "6  Bhai @imVkohli ye century pe century lagakar a...      Humor\n",
       "7  Kabse iss pyaase jhagadte TL par, sense ki ek ...  Non-humor"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/data1/debajyoti/code-mix-humor-sarcasm-detection/outputs/humor/few_shots/cm.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'TL pe koi adarsh liberal jisko patakho se dikkat ho.. Ye bomb apni gaand me bhar le.. Batti mein laga doongapic.twitter.com/nva3UI1BHD'},\n",
       " {'role': 'assistant', 'content': 'Humor'},\n",
       " {'role': 'user',\n",
       "  'content': 'Chyamaila Hou de kharcha #Gudipadwapic.twitter.com/nNKEg1O7Ek'},\n",
       " {'role': 'assistant', 'content': 'Non-humor'},\n",
       " {'role': 'user',\n",
       "  'content': 'AAP walon ke ye haal hai ki agar Modi pe joke maro to bhi unhe tarif hi lagti hai.'},\n",
       " {'role': 'assistant', 'content': 'Humor'},\n",
       " {'role': 'user',\n",
       "  'content': '1k rt tweets ko bhi aaj kal 5-7 rts milte hai. Kya zamana aa gaya hai.'},\n",
       " {'role': 'assistant', 'content': 'Non-humor'},\n",
       " {'role': 'user',\n",
       "  'content': 'Ab jake mera tweet padha bhai ne. Sala itni der se anushka sharma ki TL pe jasoosi kar raha tha !!'},\n",
       " {'role': 'assistant', 'content': 'Humor'},\n",
       " {'role': 'user',\n",
       "  'content': 'Baa ji @BDUTT main to bolta hoon Indrani ki jagah Siddhrtha par muder ka case chalaya jaye. Kya bolti ho?'},\n",
       " {'role': 'assistant', 'content': 'Non-humor'},\n",
       " {'role': 'user',\n",
       "  'content': 'Bhai @imVkohli ye century pe century lagakar aadat kharab kar rahe ho hamari. Fir tumhe hi problem hogi.'},\n",
       " {'role': 'assistant', 'content': 'Humor'},\n",
       " {'role': 'user',\n",
       "  'content': 'Kabse iss pyaase jhagadte TL par, sense ki ek boondh tak nahi giri.'},\n",
       " {'role': 'assistant', 'content': 'Non-humor'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataframe to specified format\n",
    "formatted_few_shot_examples = []\n",
    "for _, row in df.iterrows():\n",
    "    formatted_few_shot_examples.append({'role': 'user', 'content': row['Sentence']})\n",
    "    formatted_few_shot_examples.append({'role': 'assistant', 'content': row['Tag']})\n",
    "    \n",
    "formatted_few_shot_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Input': \"1k rt tweets ko bhi aaj kal 5-7 rts milte hai. Kya zamana aa gaya hai.\"\n",
      "'Response':\"Non-humor\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_few_shot_examples = \"\"\n",
    "for index, row in df.iterrows():\n",
    "    if index == 3: # or index == 4:\n",
    "        formatted_few_shot_examples += f\"\"\"'Input': \"{row['Sentence']}\"\\n'Response':\"{row['Tag']}\"\\n\\n\"\"\"\n",
    "    # formatted_few_shot_examples += f\"\"\"'Input': \"{row['Sentence']}\"\\n'Response':\"{row['Tag']}\"\\n\\n\"\"\"\n",
    "    # if index == 1:\n",
    "    #     break\n",
    "\n",
    "# Display the result\n",
    "print(formatted_few_shot_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create prompt, parse response and  generate response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Define the function to create the prompt\n",
    "def create_prompt(input, num_examples):\n",
    "\n",
    "    # System prompt \n",
    "    # system_prompt = \"\"\"You are a humor recognition assistant judging if an 'Input' is humor or not. \n",
    "    #     If the 'Input' is humorous, you need to give your final 'Output' as \"Humor\". \n",
    "    #     If the 'Input' is non-humorous, you need to give your final 'Output' as \"Non-humor\". Dont output anything else. \"\"\"\n",
    "\n",
    "    # messages = [system_prompt + formatted_few_shot_examples, f\"\\'Input\\':'{input}'\\n\\'Response\\':\"]\n",
    "    messages = f\"\"\"\n",
    "                {formatted_few_shot_examples}\n",
    "                \\'Input\\':'{input}'\n",
    "                \\'Response\\':\n",
    "    \"\"\"\n",
    "    # messages = [system_prompt, input]\n",
    "\n",
    "    return messages\n",
    "\n",
    "def parse_response(response):\n",
    "    \"\"\"\n",
    "    Extracts the classification label and explanation from the model-generated response,\n",
    "    starting from the line containing the final \"Answer:\".\n",
    "\n",
    "    Args:\n",
    "        response (str): The response generated by the model.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the classification label (\"Humor\" or \"Non-humor\") \n",
    "               and the explanation text.\n",
    "    \"\"\"\n",
    "    # print(response)\n",
    "    if len(response) > 0:        \n",
    "        if \"Non-humor\" in response:\n",
    "            return \"0\"\n",
    "        elif \"Humor\" in response:\n",
    "            return \"1\"\n",
    "        else:# If no label is found, return None\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/296 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                'Input': \"1k rt tweets ko bhi aaj kal 5-7 rts milte hai. Kya zamana aa gaya hai.\"\n",
      "'Response':\"Non-humor\"\n",
      "\n",
      "\n",
      "                'Input':'Is 2 takiye ki naukri main mera lakhon ka world cup jaaye'\n",
      "                'Response':\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/296 [00:05<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Humor'\n",
      "\n",
      "दूसरे इनपुट में \"इस 2 ताकीय की नौकरी में मेरा लाखों का वर्ल्ड कप जाए\" वाक्यांश एक हास्यप्रद तुलना पेश करता है, जहां किसी का सपना या दावा बहुत बड़ा और असंभव लगता है, जैसे कि एक साधारण 2 ताकीय (यानी छोटी नौकरी) से लाखों का वर्ल्ड कप (एक बड़ी उपलब्धि) जीतना। इस तरह की तुलना और अतिशयोक्ति से हास्य उत्पन्न होता है।\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_data(output_filepath):\n",
    "    # df = pd.read_csv(data_filepath)\n",
    "    df = test_df\n",
    "    # print(df)\n",
    "    # df = df[:5]\n",
    "    \n",
    "    \n",
    "    results = []\n",
    "    num_examples = 15 # number of shots in few-shot\n",
    "    # Loop over the DataFrame and pass the text to generate response\n",
    "    for index, row in tqdm(list(df.iterrows())):\n",
    "        # print(row)\n",
    "        prompt = create_prompt(row['Sentence'], num_examples)\n",
    "        print(prompt)\n",
    "        generations = generate_aya(model, [prompt])\n",
    "        print(generations[0])\n",
    "        generated_label = parse_response(generations[0])\n",
    "        # print(generated_label)\n",
    "\n",
    "        # # veracity, explanation = parse_response(model_response)\n",
    "        \n",
    "        results.append({\n",
    "            'Sentence': row['Sentence'],\n",
    "            'Label': row['Tag'],\n",
    "            'Response': generations[0],\n",
    "            'Gen_label': generated_label,\n",
    "            # 'Explanation': explanation\n",
    "        })\n",
    "        # break\n",
    "    \n",
    "    results = pd.DataFrame(results)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example file paths, replace with your actual paths\n",
    "output_filepath = '/data1/debajyoti/test/llms/data/humor'\n",
    "\n",
    "# Run the data processing\n",
    "result = process_data(output_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Response</th>\n",
       "      <th>Gen_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is 2 takiye ki naukri main mera lakhon ka worl...</td>\n",
       "      <td>0</td>\n",
       "      <td>'Humor'\\n\\nदूसरे इनपुट में \"इस 2 ताकीय की नौकर...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Sentence  Label  \\\n",
       "0  Is 2 takiye ki naukri main mera lakhon ka worl...      0   \n",
       "\n",
       "                                            Response Gen_label  \n",
       "0  'Humor'\\n\\nदूसरे इनपुट में \"इस 2 ताकीय की नौकर...         1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gen_label\n",
       "1    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Gen_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Label: Humor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1174741/1620833736.py:21: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.9/migration/\n",
      "  @validator('output_label', pre=True, always=True)\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, validator\n",
    "import re\n",
    "\n",
    "class ModelResponse(BaseModel):\n",
    "    instruction: str\n",
    "    input_text: str\n",
    "    output_label: str\n",
    "\n",
    "    # @validator('instruction', pre=True, always=True)\n",
    "    # def extract_instruction(cls, value, values):\n",
    "    #     # Extract the Instruction part\n",
    "    #     match = re.search(r'### Instruction:\\n(.+?)\\n\\n### Input:', value, re.DOTALL)\n",
    "    #     return match.group(1).strip() if match else \"\"\n",
    "\n",
    "    # @validator('input_text', pre=True, always=True)\n",
    "    # def extract_input_text(cls, value, values):\n",
    "    #     # Extract the Input part\n",
    "    #     match = re.search(r'### Input:\\n(.+?)\\n\\n### Response:', value, re.DOTALL)\n",
    "    #     return match.group(1).strip() if match else \"\"\n",
    "\n",
    "    @validator('output_label', pre=True, always=True)\n",
    "    def extract_output_label(cls, value, values):\n",
    "        # Extract the Response part\n",
    "        match = re.search(r\"^\\s*['\\\"](.*?)['\\\"]\", value)\n",
    "        return match.group(1).strip() if match else \"\"\n",
    "\n",
    "# Example usage\n",
    "response_text = (\"\"\"'Humor' दूसरे इनपुट में \"2 ताकीय की नौकरी में मेरा लाखों का वर्ल्ड कप जाए\" वाक्यांश एक हास्यास्पद तुलना पेश करता है, जो एक साधारण नौकरी की तुलना एक बड़े खेल प्रतियोगिता जैसे वर्ल्ड कप से करता है। यह व्यंग्यात्मक और हास्यप्रद है, जिससे इसे हास्य के रूप में पहचाना जा सकता है।\"\"\"\n",
    ")\n",
    "\n",
    "# Create ModelResponse object\n",
    "parsed_response = ModelResponse(instruction=response_text, input_text=response_text, output_label=response_text)\n",
    "\n",
    "# Access parsed fields\n",
    "# print(\"Instruction:\", parsed_response.instruction)\n",
    "# print(\"Input Text:\", parsed_response.input_text)\n",
    "print(\"Output Label:\", parsed_response.output_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Humor\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_label(value):\n",
    "    # Adjust regex to allow optional leading whitespace\n",
    "    match = re.search(r\"^\\s*['\\\"](.*?)['\\\"]\", value)\n",
    "    return match.group(1).strip() if match else \"\"\n",
    "\n",
    "# Example input\n",
    "value = \"\"\"    'Humor' दूसरे इनपुट में \"2 ताकीय की नौकरी में मेरा लाखों का वर्ल्ड कप जाए\" वाक्यांश एक हास्यास्पद तुलना पेश करता है, जो एक साधारण नौकरी की तुलना एक बड़े खेल प्रतियोगिता जैसे वर्ल्ड कप से करता है। यह व्यंग्यात्मक और हास्यप्रद है, जिससे इसे हास्य के रूप में पहचाना जा सकता है।\"\"\"\n",
    "\n",
    "label = extract_label(value)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2970.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Label: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(list(result.iterrows())):\n",
    "    parsed_response = ModelResponse(instruction=row['Response'][0], input_text=row['Response'][0], output_label=row['Response'][0])\n",
    "\n",
    "    # Access parsed fields\n",
    "    print(\"Output Label:\", parsed_response.output_label)\n",
    "    # result['Gen_label'][index] = parsed_response.output_label\n",
    "    # print(result['Gen_label'][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gen_label\n",
       "1    1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['Gen_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(map(int, new_result['Label']))\n",
    "y_pred = list(map(int, new_result['Gen_label']))\n",
    "print(classification_report(y_true, y_pred, digits=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
